HearMeNow ğŸ™ï¸

Emotion â†’ Voice. Not just words, but feeling.

HearMeNow is an accessibility-first web application that helps deaf and mute individuals communicate with natural emotion and clarity. Instead of robotic text-to-speech, HearMeNow translates emotion into voice, allowing listeners to feel the intent behind the words.

â€œFor the first time, people can not only hear my words â€” they can feel my tone.â€

ğŸ§© The Problem

Deaf and speech-impaired individuals often rely on text or monotone TTS systems, which strip away:

Emotion

Emphasis

Human intent

This makes everyday interactionsâ€”classrooms, workplaces, social settingsâ€”feel distant and misunderstood.

ğŸ’¡ The Solution

HearMeNow enables expressive communication by combining emotion detection, user-controlled tone selection, and real-time voice synthesis.

How it works:

User types a message

User selects an emotion or lets the system detect it automatically

ElevenLabs instantly speaks the message in a realistic, expressive voice

âœ¨ Key Features
ğŸ—¨ï¸ Simple Chat-Style Interface

Clean, distraction-free UI

Designed for fast, repeatable communication

ğŸ­ Emotion Selection

Manually choose emotions (e.g., happy, calm, serious, apologetic)

Or enable Smart Emotion Detection

ğŸ§  Smart Emotion Detection

Uses an OpenAI model to classify emotional tone directly from text

Automatically maps tone â†’ voice parameters

ğŸ”Š Real-Time Speech Output

WebSocket streaming from ElevenLabs

Natural timing, pacing, and expressive flow

âš¡ Quick Phrases

Save frequently used emotional phrases

Examples:

â€œThank you!â€

â€œIâ€™m sorryâ€

â€œI need helpâ€

Caret-aware insertion for fast reuse

â™¿ Accessibility Impact

Accessibility: Gives speech-impaired users an emotional voice

Empathy: Helps listeners understand how something is said, not just what

Inclusion: Supports expressive communication in classrooms, workplaces, and daily life

Human Connection: Bridges the gap between words and feelings

ğŸ—ï¸ Architecture Overview

Frontend

Vite + React + TypeScript

Tailwind CSS + shadcn/ui

Chat-style UI with emotion controls

Backend

Node.js server

WebSocket connection to ElevenLabs API

Voice Engine

ElevenLabs converts text into emotion-controlled speech in real time

AI Layer

OpenAI model classifies emotional tone from user input

ğŸ“ Repository Structure
â”œâ”€â”€ src/                 # Frontend application
â”œâ”€â”€ server/              # Backend WebSocket server
â”œâ”€â”€ public/              # Static assets
â”œâ”€â”€ README-TTS.md        # TTS documentation
â”œâ”€â”€ package.json
â”œâ”€â”€ vite.config.ts
â””â”€â”€ .gitignore           # Ignores .env.local

ğŸ” Environment Setup

Create a .env.local file (ignored by git):

OPENAI_API_KEY=your_key_here
ELEVENLABS_API_KEY=your_key_here

ğŸš€ Getting Started
npm install
npm run dev


Start the backend server separately:

node server/index.js

ğŸ› ï¸ Tech Stack

TypeScript

React (Vite)

Node.js

WebSockets

OpenAI API

ElevenLabs API

Tailwind CSS

shadcn/ui

ğŸ‘¤ Author

Hamzah Khan
Computer Science Student @ Queens College (CUNY)
Focused on accessibility, AI-powered products, and human-centered design.
